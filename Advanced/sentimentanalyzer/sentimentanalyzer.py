#Implementation of recurrent neural network to analyze movie reviews on IMDB with neon

#Download imdb dataset
from neon.data import IMDB
from neon.initializers import GlorotUniform, Uniform
from neon.layers import LSTM, Affine, Dropout, LookupTable, RecurrentSum
from neon.transforms import Logistic, Tanh, Softmax
from neon.optimizers import Adagrad
from neon.transforms import CrossEntropyMulti
from neon.layers import GeneralizedCost
from neon.callbacks import Callbacks
from neon.models import Model
from neon.transforms import Accuracy
from neon.backends import gen_backend
import preprocess_text
import cPickle
import numpy as np



gen_backend(backend='cpu', batch_size=128)

#The dataset contains huge number of words for each reviews we're gonna limit the words and truncate the length
vocab_size = 20000
max_len = 128
embedding_dim = 128
hidden_size = 128
#passing the word limit and truncated word length into IMDB dataset
imdb = IMDB(vocab_size, max_len)

# import pdb; pdb.set_trace()
#categorizing training dataset and testing dataset
train_set = imdb.train_iter
test_set = imdb.test_iter
valid_set = imdb.test_iter
#Model specification
#Initialization


init_glorot = GlorotUniform()
init_uniform = Uniform(-0.1/128, 0.1/128)

#Following are the list of layers we are gonna implement in out network


layers = [
    LookupTable(vocab_size=vocab_size, embedding_dim=128, init=init_uniform),
    LSTM(output_size=128, init=init_glorot, activation=Tanh(),
         gate_activation=Logistic(), reset_cells=True),
    RecurrentSum(),
    Dropout(keep=0.5),
    Affine(nout=2, init=init_glorot, bias=init_glorot, activation=Softmax())
]

#cost optimizer and callbacks


cost = GeneralizedCost(costfunc=CrossEntropyMulti(usebits=True))
optimizer = Adagrad(learning_rate=0.01)


num_epochs = 2
fname = 'imdb_lstm_model'

model = Model(layers=layers)

callbacks = Callbacks(model, eval_set=valid_set, eval_freq=num_epochs,
                      serialize=1, save_path=fname + '.pickle')


#Train Model


model.fit(train_set, optimizer=optimizer, num_epochs=num_epochs, cost=cost, callbacks=callbacks)

#Evaluation model on the Accuracey metric


print "Test Accuracy - ", 100 * model.eval(test_set,metric=Accuracy())
print "Train Accuracy - ", 100 * model.eval(train_set,metric=Accuracy())



#Allowing users to enter input 

# setup buffers before accepting reviews
xbuf = np.zeros((sentence_length, 1), dtype=np.int32)  # host buffer
xdev = be.zeros((sentence_length, 1), dtype=np.int32)  # device buffer

# tags for text pre-processing
oov = 2
start = 1
index_from = 3
pad_char = 0

# load dictionary from file (generated by prepare script)
vocab, rev_vocab = cPickle.load(open(fname + '.vocab', 'rb'))

while True:
    line = raw_input('Enter a Review from testData.tsv file: \n')

    # clean the input
    tokens = preprocess_text.clean_string(line).strip().split()

    # convert strings to one-hot. Check for oov and add start
    sent = [len(vocab) + 1 if t not in vocab else vocab[t] for t in tokens]
    sent = [start] + [w + index_from for w in sent]
    sent = [oov if w >= vocab_size else w for w in sent]

    # pad sentences
    xbuf[:] = 0
    trunc = sent[-sentence_length:]
    xbuf[-len(trunc):, 0] = trunc  # load list into numpy array
    xdev[:] = xbuf  # load numpy array into device tensor

    # run the sentence through the model
    y_pred = model_new.fprop(xdev, inference=True)

    print '-' * 100
    print "Sentence encoding: {0}".format(xbuf.T)
    print "\nPrediction: {:.1%} negative, {:.1%} positive".format(y_pred.get()[0,0], y_pred.get()[1,0])
    print '-' * 100
